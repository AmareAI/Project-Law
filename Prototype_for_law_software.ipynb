{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install requests_html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYH30PdcQZkU",
        "outputId": "ef3fcdff-e8d9-4765-f31f-05c267d05a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests_html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.27.1)\n",
            "Collecting pyquery (from requests_html)\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent (from requests_html)\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parse (from requests_html)\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bs4 (from requests_html)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting w3lib (from requests_html)\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests_html)\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2021 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (2022.12.7)\n",
            "Collecting importlib-metadata>=1.4 (from pyppeteer>=0.0.14->requests_html)\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer>=0.0.14->requests_html)\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.65.0)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.26.15)\n",
            "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests_html)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests_html) (4.11.2)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (4.9.2)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests_html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests_html) (2.4.1)\n",
            "Building wheels for collected packages: bs4, parse\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=50e5d60f5902e56ba50641c4b4d33b770753e755a90614cd76ca887bdbba04e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24570 sha256=3c25e3bc6a1ee9f0198f67801155cd74f802e211ab42fc994822b771dbe27ec3\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4b/f0/eaf5a8de646d8676dc25caa01949b9f9d883b8fa2efb435bc3\n",
            "Successfully built bs4 parse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from requests_html import HTML\n",
        "from requests_html import HTMLSession\n"
      ],
      "metadata": {
        "id": "jZCoJq8CQa93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_source(url):\n",
        "    \"\"\"Return the source code for the provided URL. \n",
        "\n",
        "    Args: \n",
        "        url (string): URL of the page to scrape.\n",
        "\n",
        "    Returns:\n",
        "        response (object): HTTP response object from requests_html. \n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        session = HTMLSession()\n",
        "        response = session.get(url)\n",
        "        return response\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "I9Q5DgFpQ9si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feed(url):\n",
        "    \"\"\"Return a Pandas dataframe containing the RSS feed contents.\n",
        "\n",
        "    Args: \n",
        "        url (string): URL of the RSS feed to read.\n",
        "\n",
        "    Returns:\n",
        "        df (dataframe): Pandas dataframe containing the RSS feed contents.\n",
        "    \"\"\"\n",
        "    \n",
        "    response = get_source(url)\n",
        "    \n",
        "    df = pd.DataFrame(columns = ['author', 'title', 'link', 'id', 'updated', 'published', 'summary', 'content'])\n",
        "\n",
        "    with response as r:\n",
        "        entries = r.html.find(\"entry\", first=False)\n",
        "\n",
        "        for entry in entries:        \n",
        "\n",
        "            author = entry.find('author', first=True).text\n",
        "            title = entry.find('title', first=True).text\n",
        "            link = entry.find('link', first=True).html\n",
        "            id = entry.find('id', first=True).text\n",
        "            updated = entry.find('updated', first=True).text\n",
        "            published = entry.find('published', first=True).text\n",
        "            summary = entry.find('summary', first=True).text\n",
        "            content = entry.find('content', first=True).text\n",
        "\n",
        "            row = {'author': author, 'title': title, 'link': link, 'id': id, 'updated': updated, 'published': published, 'summary': summary, 'content': content}\n",
        "            df = df.append(row, ignore_index=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "9-x9j4KRSh9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.judiciary.uk/judgments/feed/\""
      ],
      "metadata": {
        "id": "gQywXNv_S016"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_feed(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "D9T8WR5XS4rA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}